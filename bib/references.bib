@article{Beck2016Visual,
  abstract = {Bibiographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.},
  author = {Beck, Fabian and Koch, Sebastian and Weiskopf, Daniel},
  doi = {10.1109/TVCG.2015.2467757},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  keywords = {type:system, visual_analytics, sparklines, information_retrieval, clustering, literature_browser},
  number = {01},
  publisher = {IEEE},
  volume = {22},
  series = {TVCG},
  title = {Visual Analysis and Dissemination of Scientific Literature Collections with {SurVis}},
  url = {http://www.visus.uni-stuttgart.de/uploads/tx_vispublications/vast15-survis.pdf},
  year = {2016}
}

@misc{huang2023segment,
      keywords = {type:Segment Anything Model, Medical Image Segmentation,Medical Object Perception},
      title={Segment Anything Model for Medical Images?}, 
      author={Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen, Haozhe Chi, Xindi Hu, Deng-Ping Fan, Fajin Dong, Dong Ni},
      year={2023},
      eprint={2304.14660},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@InProceedings{Long_2015_CVPR,
	keywords = {type:semantic segmentation, fully convolutional networks}
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	title = {Fully Convolutional Networks for Semantic Segmentation},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2015}
}

@misc{badrinarayanan2015segnet,
      Keywords = {type:Deep Convolutional Neural Networks, Semantic Pixel-Wise Segmentation, Indoor Scenes, Road Scenes, Encoder, Decoder, Pooling, Upsampling}	
      title={SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}, 
      author={Vijay Badrinarayanan and Alex Kendall and Roberto Cipolla},
      year={2015},
      eprint={1511.00561},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{Ronneberger2015Unet,
author={Ronneberger, Olaf
and Fischer, Philipp
and Brox, Thomas},
editor="Navab, Nassir
and Hornegger, Joachim
and Wells, William M.
and Frangi, Alejandro F.",
keywords={type:deep learning, convolutional networks, biomedical image segmentation, data augmentation, cell tracking}
title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
year={2015},
publisher="Springer International Publishing",
address="Cham",
pages="234--241",
abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
isbn="978-3-319-24574-4"
}

@INPROCEEDINGS{7785132,
  keywords={type:Convolutional Neural Networks, Medical Image Analysis, Volumetric Segmentation, MRI Volumes, Dice coefficient},
  author={Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
  booktitle={2016 Fourth International Conference on 3D Vision (3DV)}, 
  title={V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation}, 
  year={2016},
  pages={565-571},
  doi={10.1109/3DV.2016.79}}

@article{Isensee2021nnUNet,
   keywords={type:semantic segmentation, deep learning, biomedical imaging, AutoML, nnU-Net},
   author = {Isensee, F. and Jaeger, P. F. and Kohl, S. A. A. and Petersen, J. and Maier-Hein, K. H.},
   title = {nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation},
   journal = {Nat Methods},
   volume = {18},
   number = {2},
   pages = {203-211},
   note = {1548-7105
Isensee, Fabian
Jaeger, Paul F
Kohl, Simon A A
Petersen, Jens
Maier-Hein, Klaus H
Orcid: 0000-0002-6626-2463
Journal Article
Research Support, Non-U.S. Gov't
United States
2020/12/09
Nat Methods. 2021 Feb;18(2):203-211. doi: 10.1038/s41592-020-01008-z. Epub 2020 Dec 7.},
   abstract = {Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.},
   keywords = {Algorithms
*Deep Learning
Image Processing, Computer-Assisted/methods
Neural Networks, Computer},
   ISSN = {1548-7091},
   DOI = {10.1038/s41592-020-01008-z},
   year = {2021},
   type = {Journal Article}
}

@ARTICLE{7913730,
  keywords={type:semantic segmentation, Convolutional neural networks, atrous convolution, conditional random fields},
  author={Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}, 
  year={2017},
  volume={40},
  number={4},
  pages={834-848},
  doi={10.1109/TPAMI.2017.2699184}}

@InProceedings{Chen_2018_ECCV,
keywords={type:Semantic segmentation, spatial pyramid pooling, encoder-decoder, depthwise separable convolution},
author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
title = {Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@InProceedings{Wang2020AxialDeepLab,
keywords={type:self-attention, bottom-up panoptic segmentation},
author={Wang, Huiyu
and Zhu, Yukun
and Green, Bradley
and Adam, Hartwig
and Yuille, Alan
and Chen, Liang-Chieh},
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation",
booktitle="Computer Vision -- ECCV 2020",
year={2020},
publisher="Springer International Publishing",
address="Cham",
pages="108--126",
abstract="Convolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions. Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D self-attentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8{\%} PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is {\$}{\$}3.8{\backslash}times {\$}{\$}parameter-efficient and {\$}{\$}27{\backslash}times {\$}{\$}computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes.",
isbn="978-3-030-58548-8"
}

@InProceedings{Cao2023SwinUnet,
keywords={type:medical image segmentation, convolutional neural networks, Transformer, semantic information interaction, U-net},
author={Cao, Hu
and Wang, Yueyue
and Chen, Joy
and Jiang, Dongsheng
and Zhang, Xiaopeng
and Tian, Qi
and Wang, Manning},
editor="Karlinsky, Leonid
and Michaeli, Tomer
and Nishino, Ko",
title="Swin-Unet: Unet-Like Pure Transformer for Medical Image Segmentation",
booktitle="Computer Vision -- ECCV 2022 Workshops",
year={2023},
publisher="Springer Nature Switzerland",
address="Cham",
pages="205--218",
abstract="In the past few years, convolutional neural networks (CNNs) have achieved milestones in medical image analysis. In particular, deep neural networks based on U-shaped architecture and skip-connections have been widely applied in various medical image tasks. However, although CNN has achieved excellent performance, it cannot learn global semantic information interaction well due to the locality of convolution operation. In this paper, we propose Swin-Unet, which is an Unet-like pure Transformer for medical image segmentation. The tokenized image patches are fed into the Transformer-based U-shaped Encoder-Decoder architecture with skip-connections for local-global semantic feature learning. Specifically, we use a hierarchical Swin Transformer with shifted windows as the encoder to extract context features. And a symmetric Swin Transformer-based decoder with a patch expanding layer is designed to perform the up-sampling operation to restore the spatial resolution of the feature maps. Under the direct down-sampling and up-sampling of the inputs and outputs by {\$}{\$}4{\{}{\backslash}times {\}}{\$}{\$}4{\texttimes}, experiments on multi-organ and cardiac segmentation tasks demonstrate that the pure Transformer-based U-shaped Encoder-Decoder network outperforms those methods with full-convolution or the combination of transformer and convolution. The codes have been publicly available at the link (https://github.com/HuCaoFighting/Swin-Unet).",
isbn="978-3-031-25066-8"
}

@misc{kirillov2023segment,
      keywords={type:image segmentation, pre-training, foundation models, promptable segmentation, data engine, privacy, computer vision},
      title={Segment Anything}, 
      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Dollár and Ross Girshick},
      year={2023},
      eprint={2304.02643},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
